{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kriti1400/Implexis-AI/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmNrvSxwXLvI"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import sklearn\n",
        "import spacy\n",
        "import seaborn as sns \n",
        "import re\n",
        "import nltk\n",
        "import wordcloud\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['font.family'] = 'sans-serif' \n",
        "plt.rcParams['font.serif'] = 'Ubuntu' \n",
        "plt.rcParams['font.monospace'] = 'Ubuntu Mono' \n",
        "plt.rcParams['font.size'] = 14 \n",
        "plt.rcParams['axes.labelsize'] = 12 \n",
        "plt.rcParams['axes.labelweight'] = 'bold' \n",
        "plt.rcParams['axes.titlesize'] = 12 \n",
        "plt.rcParams['xtick.labelsize'] = 12 \n",
        "plt.rcParams['ytick.labelsize'] = 12 \n",
        "plt.rcParams['legend.fontsize'] = 12 \n",
        "plt.rcParams['figure.titlesize'] = 12 \n",
        "plt.rcParams['image.cmap'] = 'jet' \n",
        "plt.rcParams['image.interpolation'] = 'none' \n",
        "plt.rcParams['figure.figsize'] = (20, 20) \n",
        "plt.rcParams['axes.grid']=False\n",
        "import string\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "plt.rcParams['lines.linewidth'] = 2 \n",
        "plt.rcParams['lines.markersize'] = 8\n",
        "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',\n",
        "'xkcd:scarlet']\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3s8Zx-A8jgZ",
        "outputId": "e18cd620-f452-4d0a-c84d-199dd1919e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.3-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 27.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset\n",
        "df = pd.read_csv('liberal_conservative_Reddit.csv')\n",
        "#df = df.loc[0: 1000]\n",
        "#keep the first two columns only\n",
        "df = df.drop(columns = ['Score' , 'Id' , 'Subreddit' , 'URL', 'Num of Comments', 'Text', 'Date Created'])\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "_iuKLbAiTQXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e8513338-070a-4aba-a1a7-f02d6c456831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 12,854\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Title Political Lean\n",
              "5674   CALL TO ACTION ----> ABOLISH the Filibuster an...        Liberal\n",
              "10884                                     Reconciliation   Conservative\n",
              "9890                            Christ Over Conservatism   Conservative\n",
              "403        Excerpt from Smedley Butler's War is a Racket        Liberal\n",
              "563    Bice: U.S. Senate candidate Mandela Barnes fav...        Liberal\n",
              "969    They're already far in the \"negative credit\" zone        Liberal\n",
              "10796  Milton Friedman -- Winner of the \"Libertarian ...   Conservative\n",
              "7121   Statement by President Joe Biden on the House ...        Liberal\n",
              "7708                             Feminist Yearbook Quote        Liberal\n",
              "9857   On The Border: Agents Seize $600K Of Meth On M...   Conservative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f69bd752-4d9e-4dac-8fa4-868defc907c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Political Lean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5674</th>\n",
              "      <td>CALL TO ACTION ----&gt; ABOLISH the Filibuster an...</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10884</th>\n",
              "      <td>Reconciliation</td>\n",
              "      <td>Conservative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9890</th>\n",
              "      <td>Christ Over Conservatism</td>\n",
              "      <td>Conservative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>Excerpt from Smedley Butler's War is a Racket</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>Bice: U.S. Senate candidate Mandela Barnes fav...</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>969</th>\n",
              "      <td>They're already far in the \"negative credit\" zone</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10796</th>\n",
              "      <td>Milton Friedman -- Winner of the \"Libertarian ...</td>\n",
              "      <td>Conservative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7121</th>\n",
              "      <td>Statement by President Joe Biden on the House ...</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7708</th>\n",
              "      <td>Feminist Yearbook Quote</td>\n",
              "      <td>Liberal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9857</th>\n",
              "      <td>On The Border: Agents Seize $600K Of Meth On M...</td>\n",
              "      <td>Conservative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f69bd752-4d9e-4dac-8fa4-868defc907c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f69bd752-4d9e-4dac-8fa4-868defc907c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f69bd752-4d9e-4dac-8fa4-868defc907c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXhTWKzM8gqc",
        "outputId": "0cdf253f-22ee-4c68-e64b-00669d53e683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace Political Lean values with 0 for Liberal and 1 for Conservative\n",
        "for i in range(len(df['Political Lean'])):\n",
        "  df['Political Lean'] = df['Political Lean'].replace(to_replace=['Liberal', 'Conservative'], value=[0, 1])\n",
        "\n",
        "df.head()\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "Tfc2Cav3E6tH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c3b00134-672c-4cb3-f339-df75328c9cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Title  Political Lean\n",
              "12849  Ron Paul’s Spirited Defense of WikiLeaks & Fre...               1\n",
              "12850  “Anarcho-capitalism, in my opinion, is a doctr...               1\n",
              "12851  Mises Wiki is a wiki project dedicated to the ...               1\n",
              "12852  Fireman Protection Monopoly - Is This Failed C...               1\n",
              "12853      Can this Wikipedia Article be Better Written?               1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f729de22-dd7c-4bbc-af2f-110812775cd0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Political Lean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12849</th>\n",
              "      <td>Ron Paul’s Spirited Defense of WikiLeaks &amp; Fre...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12850</th>\n",
              "      <td>“Anarcho-capitalism, in my opinion, is a doctr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12851</th>\n",
              "      <td>Mises Wiki is a wiki project dedicated to the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12852</th>\n",
              "      <td>Fireman Protection Monopoly - Is This Failed C...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12853</th>\n",
              "      <td>Can this Wikipedia Article be Better Written?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f729de22-dd7c-4bbc-af2f-110812775cd0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f729de22-dd7c-4bbc-af2f-110812775cd0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f729de22-dd7c-4bbc-af2f-110812775cd0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles = df['Title'].values\n",
        "labels = df['Political Lean'].values"
      ],
      "metadata": {
        "id": "STJWpq3vAlb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InYgKD6NACPK",
        "outputId": "5a534716-18aa-42fc-a723-4a51c6c3e857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original title.\n",
        "print(' Original: ', titles[0])\n",
        "\n",
        "# Print the title split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(titles[0]))\n",
        "\n",
        "# Print the title mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(titles[0])))\n",
        "\n",
        "## find token with maximum id value\n",
        "# max_token_id = -1\n",
        "# for title in titles:\n",
        "#   ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(title))\n",
        "#   for id in ids:\n",
        "#     if id > max_token_id:\n",
        "#       max_token_id = id\n",
        "# print(max_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiOQ_dgJAfWw",
        "outputId": "49a6d4a9-aa23-4696-b56d-c7722b0083a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.\n",
            "Tokenized:  ['no', 'matter', 'who', 'someone', 'is', ',', 'how', 'they', 'look', 'like', ',', 'what', 'language', 'they', 'speak', ',', 'what', 'they', 'wear', ',', 'remember', 'the', 'human', '.', 'for', 'the', 'sake', 'of', 'humanity', ',', 'the', 'working', 'class', 'can', 'and', 'must', 'unite', 'across', 'all', 'arbitrary', 'boundaries', '.']\n",
            "Token IDs:  [2053, 3043, 2040, 2619, 2003, 1010, 2129, 2027, 2298, 2066, 1010, 2054, 2653, 2027, 3713, 1010, 2054, 2027, 4929, 1010, 3342, 1996, 2529, 1012, 2005, 1996, 8739, 1997, 8438, 1010, 1996, 2551, 2465, 2064, 1998, 2442, 15908, 2408, 2035, 15275, 7372, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every title...\n",
        "for sent in titles:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum title length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLiCPO--BWu9",
        "outputId": "52dfdc72-d07e-49e3-bcf7-692414bee0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note\n",
        "Change the \"max_length\" to the next power of 2 in accordance with max_len"
      ],
      "metadata": {
        "id": "t5mLPZzL-4V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "attention_masks = [] \n",
        "\n",
        "# For every sentence...\n",
        "for sent in titles:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens. Attention masks basically pad shorter sentence to match the lenght of the longest sequence\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)     # tokens have been mapped to ids so this is now int\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "# input_ids is a (N x L) tensor, N = number of samples (~12k), L = max sentence length (128)\n",
        "print('Original: ', titles[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Token IDs:', labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHv_l4BiBrGX",
        "outputId": "1b819893-e9ec-4850-9852-bcccfe47cccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.\n",
            "Token IDs: tensor([  101,  2053,  3043,  2040,  2619,  2003,  1010,  2129,  2027,  2298,\n",
            "         2066,  1010,  2054,  2653,  2027,  3713,  1010,  2054,  2027,  4929,\n",
            "         1010,  3342,  1996,  2529,  1012,  2005,  1996,  8739,  1997,  8438,\n",
            "         1010,  1996,  2551,  2465,  2064,  1998,  2442, 15908,  2408,  2035,\n",
            "        15275,  7372,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Token IDs: tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # get the vocab length\n",
        "\n",
        "# vocab = set(''.join(titles))\n",
        "# # vocab = [\"Title\", \"Political Lean\"]\n",
        "# # for title in titles:\n",
        "# #   vocab += tokenizer.tokenize(title)\n",
        "# # print(vocab)\n",
        "#print(vocab)"
      ],
      "metadata": {
        "id": "jXqwAqWQbkD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab = sorted(vocab)\n",
        "# vocab_len = len(vocab)\n",
        "# print(vocab_len)\n",
        "\n",
        "# vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
        "# print(vocab_stoi)\n",
        "# vocab_itos = {i: s for i, s in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "a5JRmiqJcCmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_dataset_len = len(titles)\n",
        "\n",
        "# vocab = sorted(set(titles))       # Unique characters\\\n",
        "# vocab_len = len(vocab)\n",
        "# vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
        "# vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
        "# print(vocab_len)"
      ],
      "metadata": {
        "id": "pPrtlKW6Iy3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_dataset_len = len(vocab)\n",
        "\n",
        "# # vocab = sorted(set(vocab))       # Unique characters\\\n",
        "# vocab_len = len(vocab)\n",
        "# vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
        "# vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
        "# print(vocab_len)"
      ],
      "metadata": {
        "id": "6-I0UQPQQhBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "DATA_SIZE = len(dataset)\n",
        "train_val_size = int(0.9 * DATA_SIZE)\n",
        "train_size = int(0.9 * train_val_size)\n",
        "val_size = train_val_size  - train_size \n",
        "test_size = DATA_SIZE - train_val_size\n",
        "\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_val_dataset, test_dataset = random_split(dataset, [train_val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "# #change the seed...\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHtgmcgJ6mAz",
        "outputId": "8320c44e-5d47-4379-9bf2-32b927be6a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10,411 training samples\n",
            "1,157 validation samples\n",
            "1,286 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "#prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Lx5tNDBV8H3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Model"
      ],
      "metadata": {
        "id": "2h7qkVehTnKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "MAX_ID = 30000 # Not sure, veify but I think this is the max ID number, I think its ~30,000\n",
        "\n",
        "\n",
        "# Input size is encoding size \n",
        "# Output size is 2 for binary classification\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_size , hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(MAX_ID, embedding_size)\n",
        "\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers=1, batch_first=True)\n",
        "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)\n",
        "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.embedding(input)       # Convert (B x L) to (B x L x E)\n",
        "        # input = (batch_size, chunk_size, vocab_len)\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        output = self.output_layer(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "LXZLu0_WTmra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def random_chunk(chunk_len=50):\n",
        "#     # Return a random subsequence from our dataset\n",
        "#     start_index = random.randint(0, vocab_len - chunk_len - 1)\n",
        "#     end_index = start_index + chunk_len + 1\n",
        "#     return df[start_index:end_index]\n",
        "\n",
        "# def text_to_tensor(text, vocab=vocab):\n",
        "#     # Return a tensor containing the indices of characters in our text\n",
        "#     indices = [vocab_stoi[ch] for ch in text]\n",
        "#     print(text)\n",
        "#     return torch.tensor(indices)\n",
        "\n",
        "# def one_hot(inp):\n",
        "#     # Convert our 1D tensor of indices into a 2D tensor of one-hot encoding for each index\n",
        "#     return F.one_hot(inp, vocab_len).float()\n",
        "\n",
        "# def random_training_set(chunk_len=50, batch_size=32):\n",
        "#     # Get a batch_size tensor of random text chunks and the corresponding output\n",
        "#     # Initialize both to zeros\n",
        "#     inp = torch.zeros([batch_size, chunk_len]).long()\n",
        "#     target = torch.zeros([batch_size, chunk_len]).long()\n",
        "#     # For each item for our batch\n",
        "#     for i in range(batch_size):\n",
        "#         # Get random chunk and insert into the corresponding row in tensor\n",
        "#         chunk = random_chunk(chunk_len)\n",
        "#         inp[i:i+1] = text_to_tensor(chunk[:-1])   # omit the last token\n",
        "#         target[i:i+1]  = text_to_tensor(chunk[1:]) # omit the first token\n",
        "#     return inp, target\n"
      ],
      "metadata": {
        "id": "uTNg0PoUV6dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inp, target = random_training_set()\n",
        "# #inp, target = train_dataset['Title'].values, train_dataset['Political Lean'].values\n",
        "# #inp, target = titles, labels\n",
        "# print(one_hot(inp).shape)\n",
        "# print(target.shape)"
      ],
      "metadata": {
        "id": "FgFatmpZc_0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, num_iters=2000, batch_size=1, lr=0.004):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for it in range(num_iters):     # Epochs\n",
        "        loss_value = 0.0\n",
        "        # For batch in train_dataloader loop, put everythign below inside of that\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "          b_input_ids = batch[0].to(device)           # (B x L) tensor, B = batch size (32), L = max sentence length (128)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)   # Ground truth (B x 1) or (B x 2), verify\n",
        "          optimizer.zero_grad()      \n",
        "          hidden = model.init_hidden(batch_size).to(device)\n",
        "          for i in range(b_input_ids.shape[-1]):\n",
        "            output, hidden = model(b_input_ids[:,i:i+1], hidden)\n",
        "\n",
        "          #\n",
        "          \n",
        "          loss = criterion(output[:,0], b_labels.flatten())\n",
        "\n",
        "          loss_value += loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        print(loss_value)"
      ],
      "metadata": {
        "id": "jhdXKHKTde30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 64\n",
        "rnn = RNN(16, n_hidden, 2)\n",
        "batch_size = 32\n",
        "train(rnn, num_iters=10, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "9PPK6GSddn-L",
        "outputId": "bd526890-bb35-447f-8798-4975c1fe475e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-fdb0d71b76e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-3e464835485e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_iters, batch_size, lr)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4560b606f1cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# Convert (B x L) to (B x L x E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# input = (batch_size, chunk_size, vocab_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RNN_TANH'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RNN_RELU'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    224\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 32, 64), got [1, 16, 64]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Data"
      ],
      "metadata": {
        "id": "85v-zAi46vGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Split Data into Training and Test Sets\n",
        "Could try k-fold validation"
      ],
      "metadata": {
        "id": "786b2Emh78fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading Data into DataLoader\n",
        "Allows us ot use an iterator instead of for loops"
      ],
      "metadata": {
        "id": "fOYCHvm6_KHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWH9fYoo_Xes",
        "outputId": "4c71b518-7f00-44a4-8e98-b74df3f6ceeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M9kFmhWAkgl",
        "outputId": "45394969-917e-406a-f77b-178eb4801608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Note\n",
        "Learning rate might need to be changed depending on the data"
      ],
      "metadata": {
        "id": "biifG5-_A2Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUQ_I1M3AwlC",
        "outputId": "f6e2a8de-8002-40b9-a32c-3e01b2cf51d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Note\n",
        "Can change num of epochs "
      ],
      "metadata": {
        "id": "pHRPVEO-BgXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "koz849C_BdAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "n8BvOmzlB4pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "kTwl6cDjCAwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)           # (B x L) tensor, B = batch size (32), L = max sentence length (128)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)              # Ground truth (B x 1) or (B x 2), verify\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "        \n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = output.loss\n",
        "            logits = output.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JthcOH90COWB",
        "outputId": "9308716a-e8da-46bf-c5ca-0a91edae125d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    326.    Elapsed: 0:00:37.\n",
            "  Batch    80  of    326.    Elapsed: 0:01:13.\n",
            "  Batch   120  of    326.    Elapsed: 0:01:50.\n",
            "  Batch   160  of    326.    Elapsed: 0:02:26.\n",
            "  Batch   200  of    326.    Elapsed: 0:03:02.\n",
            "  Batch   240  of    326.    Elapsed: 0:03:39.\n",
            "  Batch   280  of    326.    Elapsed: 0:04:15.\n",
            "  Batch   320  of    326.    Elapsed: 0:04:52.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:04:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.49\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    326.    Elapsed: 0:00:36.\n",
            "  Batch    80  of    326.    Elapsed: 0:01:13.\n",
            "  Batch   120  of    326.    Elapsed: 0:01:49.\n",
            "  Batch   160  of    326.    Elapsed: 0:02:25.\n",
            "  Batch   200  of    326.    Elapsed: 0:03:01.\n",
            "  Batch   240  of    326.    Elapsed: 0:03:38.\n",
            "  Batch   280  of    326.    Elapsed: 0:04:14.\n",
            "  Batch   320  of    326.    Elapsed: 0:04:50.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:04:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    326.    Elapsed: 0:00:36.\n",
            "  Batch    80  of    326.    Elapsed: 0:01:13.\n",
            "  Batch   120  of    326.    Elapsed: 0:01:49.\n",
            "  Batch   160  of    326.    Elapsed: 0:02:25.\n",
            "  Batch   200  of    326.    Elapsed: 0:03:01.\n",
            "  Batch   240  of    326.    Elapsed: 0:03:38.\n",
            "  Batch   280  of    326.    Elapsed: 0:04:14.\n",
            "  Batch   320  of    326.    Elapsed: 0:04:50.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:04:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.51\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    326.    Elapsed: 0:00:36.\n",
            "  Batch    80  of    326.    Elapsed: 0:01:13.\n",
            "  Batch   120  of    326.    Elapsed: 0:01:49.\n",
            "  Batch   160  of    326.    Elapsed: 0:02:25.\n",
            "  Batch   200  of    326.    Elapsed: 0:03:01.\n",
            "  Batch   240  of    326.    Elapsed: 0:03:38.\n",
            "  Batch   280  of    326.    Elapsed: 0:04:14.\n",
            "  Batch   320  of    326.    Elapsed: 0:04:50.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:04:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:20:53 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4PcQxtiOQ-lj",
        "outputId": "d78e2be0-ab95-4d45-d36f-7f72ed94eb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.57         0.49           0.77       0:04:57         0:00:18\n",
              "2               0.42         0.46           0.80       0:04:55         0:00:18\n",
              "3               0.31         0.51           0.79       0:04:55         0:00:18\n",
              "4               0.23         0.55           0.79       0:04:55         0:00:18"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65529c53-97d9-46b4-9a42-85921ab384a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:04:57</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:04:55</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0:04:55</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0:04:55</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65529c53-97d9-46b4-9a42-85921ab384a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65529c53-97d9-46b4-9a42-85921ab384a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65529c53-97d9-46b4-9a42-85921ab384a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "T_b2JyuzRM0P",
        "outputId": "3a32ee3c-41e0-4858-fbb0-92b6ce99df44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "df = df.loc[0: 1000]\n",
        "#keep the first two columns only\n",
        "df = df.drop(columns = ['Score' , 'Id' , 'Subreddit' , 'URL', 'Num of Comments', 'Text', 'Date Created'])\n",
        "# Replace Political Lean values with 0 for Liberal and 1 for Conservative\n",
        "for i in range(len(df['Political Lean'])):\n",
        "  df['Political Lean'] = df['Political Lean'].replace(to_replace=['Liberal', 'Conservative'], value=[0, 1])\n",
        "titles = df['Title'].values\n",
        "labels = df['Political Lean'].values"
      ],
      "metadata": {
        "id": "yytYJJMlVUwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "#df = pd.read_csv(\"liberal_conservative_Reddit.csv\")\n",
        "#df = df.loc[0: 1000]\n",
        "\n",
        "# # Tokenization\n",
        "# #from transformers import BertTokenizer\n",
        "# # Load the BERT tokenizer.\n",
        "# #print('Loading BERT tokenizer...')\n",
        "# #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "# input_ids = []\n",
        "# attention_masks = []\n",
        "\n",
        "\n",
        "# # For every sentence...\n",
        "# for sent in titles:\n",
        "#     # `encode_plus` will:\n",
        "#     #   (1) Tokenize the sentence.\n",
        "#     #   (2) Prepend the `[CLS]` token to the start.\n",
        "#     #   (3) Append the `[SEP]` token to the end.\n",
        "#     #   (4) Map tokens to their IDs.\n",
        "#     #   (5) Pad or truncate the sentence to `max_length`\n",
        "#     #   (6) Create attention masks for [PAD] tokens.\n",
        "#     encoded_dict = tokenizer.encode_plus(\n",
        "#                         sent,                      # Sentence to encode.\n",
        "#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "#                         max_length = 128,           # Pad & truncate all sentences.\n",
        "#                         pad_to_max_length = True,\n",
        "#                         return_attention_mask = True,   # Construct attn. masks.\n",
        "#                         return_tensors = 'pt',     # Return pytorch tensors.\n",
        "#                    )\n",
        "    \n",
        "#     # Add the encoded sentence to the list.    \n",
        "#     input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "#     # And its attention mask (simply differentiates padding from non-padding).\n",
        "#     attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# # Convert the lists into tensors.\n",
        "# input_ids = torch.cat(input_ids, dim=0)\n",
        "# attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "#prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "v8pM3Sd3RXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note:\n",
        "it says we are using the whole dataset for testing which is wrong."
      ],
      "metadata": {
        "id": "LeiXNp4AWOOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test titles...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw3C9R23VYYE",
        "outputId": "29d1724a-2bfb-4cbf-cb79-efd75fb4eed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 12,854 test titles...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles = df['Title'].values\n",
        "labels = df['Political Lean'].values\n",
        "# Replace Political Lean values with 0 for Liberal and 1 for Conservative\n",
        "for i in range(len(df['Political Lean'])):\n",
        "  df['Political Lean'] = df['Political Lean'].replace(to_replace=['Liberal', 'Conservative'], value=[0, 1])\n",
        "\n",
        "print('Positive samples: %d of %d (%.2f%%)' % (df['Political Lean'].values.sum(), len(df['Political Lean'].values), (df['Political Lean'].values.sum() / len(df['Political Lean'].values) * 100.0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-cNGcywVjQ5",
        "outputId": "3531d9f8-3554-425e-bd0d-ee23670aa362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 4535 of 12854 (35.28%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "#soft max>> scale problem\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAAQedEUYH0R",
        "outputId": "bbaee9d4-ab71-4e61-e8fe-729836f1bbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "e702yOGUYNQe",
        "outputId": "401ecda9-186d-410e-c87f-b6e87d289664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c3AUYV2yL5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE4XW_fJYXQh",
        "outputId": "d7c63db0-c188-4ead-b435-d1887f4069d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total MCC: 0.509\n"
          ]
        }
      ]
    }
  ]
}